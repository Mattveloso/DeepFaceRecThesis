#Program to detect and recognize faces based on FaceNet.
#Sources include:
# Facenet Sources
#Website
#https://github.com/ipazc/mtcnn

# -*- coding: utf-8 -*-
"""Inception-ResNet V1 model for Keras.
# Reference
http://arxiv.org/abs/1602.07261
https://github.com/davidsandberg/facenet/blob/master/src/models/inception_resnet_v1.py
https://github.com/myutwo150/keras-inception-resnet-v2/blob/master/inception_resnet_v2.py
"""
from functools import partial

from keras.models import Model
from keras.models import load_model #M
from keras.layers import Activation
from keras.layers import BatchNormalization
from keras.layers import Concatenate
from keras.layers import Conv2D
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import GlobalAveragePooling2D
from keras.layers import Input
from keras.layers import Lambda
from keras.layers import MaxPooling2D
from keras.layers import add
from keras import backend as K

import sys
import numpy as np
from numpy import genfromtxt
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import cv2
import os
import timeit
import time
sys.path.append("C:/Users/Shadow/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/code/")
sys.path.append("C:/Users/Shadow/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/model/")
sys.path.append("C:/Users/Shadow/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/weights/")
sys.path.append("C:/Users/Shadow/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/")

import mtcnn
from mtcnn import MTCNN
import PIL
from os import listdir
from os.path import isdir
from PIL import Image
from matplotlib import pyplot
from numpy import savez_compressed
from numpy import asarray
from numpy import load
from numpy import expand_dims
import numba
from numba import njit, jit

from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
import sklearn
from random import choice

detector = MTCNN()
# %% Standard imported functions
def scaling(x, scale):
	return x * scale


def conv2d_bn(x,
			  filters,
			  kernel_size,
			  strides=1,
			  padding='same',
			  activation='relu',
			  use_bias=False,
			  name=None):
	x = Conv2D(filters,
			   kernel_size,
			   strides=strides,
			   padding=padding,
			   use_bias=use_bias,
			   name=name)(x)
	if not use_bias:
		bn_axis = 1 if K.image_data_format() == 'channels_first' else 3
		bn_name = _generate_layer_name('BatchNorm', prefix=name)
		x = BatchNormalization(axis=bn_axis, momentum=0.995, epsilon=0.001,
							   scale=False, name=bn_name)(x)
	if activation is not None:
		ac_name = _generate_layer_name('Activation', prefix=name)
		x = Activation(activation, name=ac_name)(x)
	return x


def _generate_layer_name(name, branch_idx=None, prefix=None):
	if prefix is None:
		return None
	if branch_idx is None:
		return '_'.join((prefix, name))
	return '_'.join((prefix, 'Branch', str(branch_idx), name))

def _inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):
	channel_axis = 1 if K.image_data_format() == 'channels_first' else 3
	if block_idx is None:
		prefix = None
	else:
		prefix = '_'.join((block_type, str(block_idx)))
	name_fmt = partial(_generate_layer_name, prefix=prefix)

	if block_type == 'Block35':
		branch_0 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_1x1', 0))
		branch_1 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 1))
		branch_1 = conv2d_bn(branch_1, 32, 3, name=name_fmt('Conv2d_0b_3x3', 1))
		branch_2 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 2))
		branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0b_3x3', 2))
		branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0c_3x3', 2))
		branches = [branch_0, branch_1, branch_2]
	elif block_type == 'Block17':
		branch_0 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_1x1', 0))
		branch_1 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_0a_1x1', 1))
		branch_1 = conv2d_bn(branch_1, 128, [1, 7], name=name_fmt('Conv2d_0b_1x7', 1))
		branch_1 = conv2d_bn(branch_1, 128, [7, 1], name=name_fmt('Conv2d_0c_7x1', 1))
		branches = [branch_0, branch_1]
	elif block_type == 'Block8':
		branch_0 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_1x1', 0))
		branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))
		branch_1 = conv2d_bn(branch_1, 192, [1, 3], name=name_fmt('Conv2d_0b_1x3', 1))
		branch_1 = conv2d_bn(branch_1, 192, [3, 1], name=name_fmt('Conv2d_0c_3x1', 1))
		branches = [branch_0, branch_1]
	else:
		raise ValueError('Unknown Inception-ResNet block type. '
						 'Expects "Block35", "Block17" or "Block8", '
						 'but got: ' + str(block_type))

	mixed = Concatenate(axis=channel_axis, name=name_fmt('Concatenate'))(branches)
	up = conv2d_bn(mixed,
				   K.int_shape(x)[channel_axis],
				   1,
				   activation=None,
				   use_bias=True,
				   name=name_fmt('Conv2d_1x1'))
	up = Lambda(scaling,
				output_shape=K.int_shape(up)[1:],
				arguments={'scale': scale})(up)
	x = add([x, up])
	if activation is not None:
		x = Activation(activation, name=name_fmt('Activation'))(x)
	return x

def InceptionResNetV1(input_shape=(160, 160, 3), classes=128, dropout_keep_prob=0.8, weights_path=None):
	inputs = Input(shape=input_shape)
	x = conv2d_bn(inputs, 32, 3, strides=2, padding='valid', name='Conv2d_1a_3x3')
	x = conv2d_bn(x, 32, 3, padding='valid', name='Conv2d_2a_3x3')
	x = conv2d_bn(x, 64, 3, name='Conv2d_2b_3x3')
	x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)
	x = conv2d_bn(x, 80, 1, padding='valid', name='Conv2d_3b_1x1')
	x = conv2d_bn(x, 192, 3, padding='valid', name='Conv2d_4a_3x3')
	x = conv2d_bn(x, 256, 3, strides=2, padding='valid', name='Conv2d_4b_3x3')

	# 5x Block35 (Inception-ResNet-A block):
	for block_idx in range(1, 6):
		x = _inception_resnet_block(x,
									scale=0.17,
									block_type='Block35',
									block_idx=block_idx)

	# Mixed 6a (Reduction-A block):
	channel_axis = 1 if K.image_data_format() == 'channels_first' else 3
	name_fmt = partial(_generate_layer_name, prefix='Mixed_6a')
	branch_0 = conv2d_bn(x,
						 384,
						 3,
						 strides=2,
						 padding='valid',
						 name=name_fmt('Conv2d_1a_3x3', 0))
	branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))
	branch_1 = conv2d_bn(branch_1, 192, 3, name=name_fmt('Conv2d_0b_3x3', 1))
	branch_1 = conv2d_bn(branch_1,
						 256,
						 3,
						 strides=2,
						 padding='valid',
						 name=name_fmt('Conv2d_1a_3x3', 1))
	branch_pool = MaxPooling2D(3,
							   strides=2,
							   padding='valid',
							   name=name_fmt('MaxPool_1a_3x3', 2))(x)
	branches = [branch_0, branch_1, branch_pool]
	x = Concatenate(axis=channel_axis, name='Mixed_6a')(branches)

	# 10x Block17 (Inception-ResNet-B block):
	for block_idx in range(1, 11):
		x = _inception_resnet_block(x,
									scale=0.1,
									block_type='Block17',
									block_idx=block_idx)

	# Mixed 7a (Reduction-B block): 8 x 8 x 2080
	name_fmt = partial(_generate_layer_name, prefix='Mixed_7a')
	branch_0 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 0))
	branch_0 = conv2d_bn(branch_0,
						 384,
						 3,
						 strides=2,
						 padding='valid',
						 name=name_fmt('Conv2d_1a_3x3', 0))
	branch_1 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 1))
	branch_1 = conv2d_bn(branch_1,
						 256,
						 3,
						 strides=2,
						 padding='valid',
						 name=name_fmt('Conv2d_1a_3x3', 1))
	branch_2 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 2))
	branch_2 = conv2d_bn(branch_2, 256, 3, name=name_fmt('Conv2d_0b_3x3', 2))
	branch_2 = conv2d_bn(branch_2,
						 256,
						 3,
						 strides=2,
						 padding='valid',
						 name=name_fmt('Conv2d_1a_3x3', 2))
	branch_pool = MaxPooling2D(3,
							   strides=2,
							   padding='valid',
							   name=name_fmt('MaxPool_1a_3x3', 3))(x)
	branches = [branch_0, branch_1, branch_2, branch_pool]
	x = Concatenate(axis=channel_axis, name='Mixed_7a')(branches)

	# 5x Block8 (Inception-ResNet-C block):
	for block_idx in range(1, 6):
		x = _inception_resnet_block(x,
									scale=0.2,
									block_type='Block8',
									block_idx=block_idx)
	x = _inception_resnet_block(x,
								scale=1.,
								activation=None,
								block_type='Block8',
								block_idx=6)

	# Classification block
	x = GlobalAveragePooling2D(name='AvgPool')(x)
	x = Dropout(1.0 - dropout_keep_prob, name='Dropout')(x)
	# Bottleneck
	x = Dense(classes, use_bias=False, name='Bottleneck')(x)
	bn_name = _generate_layer_name('BatchNorm', prefix='Bottleneck')
	x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False,
						   name=bn_name)(x)

	# Create model
	model = Model(inputs, x, name='inception_resnet_v1')
	if weights_path is not None:
		model.load_weights(weights_path)

	return model

path1 = "C:/Users/Shadow/Documents/GitHub/DeepFaceRecThesis/"
#@jit(nogil=True,parallel=True)
def extract_face(filename, required_size=(160, 160)):
	# load image from file
	image = cv2.cvtColor(cv2.imread(filename),cv2.COLOR_BGR2RGB)
	# convert to array
	pixels = np.asarray(image)
	#start = time.time() # time measurement when necessary for optimization
	# detect faces in the image
	results = detector.detect_faces(image) #Detector has been created as a global class
	#total = time.time()-start
	#print(total)
	#extract the bounding box from the first face
	x = np.array(range(0,len(results)))
	y = np.array(range(0,len(results)))
	x2 = np.array(range(0,len(results)))
	y2 = np.array(range(0,len(results)))
	width = np.array(range(0,len(results)))
	height = np.array(range(0,len(results)))
	facez = list(np.array(range(0,len(results))))
	face_array = list(np.array(range(0,len(results))))
	for face in range(0,len(results)):
		x[face], y[face], width[face], height[face] = results[face]['box']
		# bug fix - shouldn't be necessary in latet version of MTCNN but I'll keep it just in case
		x[face], y[face] = abs(x[face]), abs(y[face])
		x2[face], y2[face] = x[face] + width[face], y[face] + height[face]
		# extract the face
		facez[face] = pixels[y[face]:y2[face], x[face]:x2[face]]
		# resize pixels to the model size
		image = PIL.Image.fromarray(facez[face])
		image = image.resize(required_size)
		face_array[face] = np.asarray(image)
	return face_array

# load images and extract faces for all images in a directory
def load_faces(directory):
	faces = list()
	# enumerate files
	for filename in os.listdir(directory):
		# path
		path = directory + filename
		# get face
		face = extract_face(path)
		# store
		for cara in face:
			faces.append(cara)
	return faces

def load_dataset(directory):
	X, y = list(), list()
	# enumerate folders, on per class, every subdir found becomes a class
	for subdir in os.listdir(directory):
		# path
		path = directory + subdir + '/'
		# skip any files that might be in the dir
		if not os.path.isdir(path):
			continue
		# load all faces in the subdirectory
		faces = load_faces(path) #M:! SLOWEST PART OF CODE! OPTIMIZE THIS!!!
		# create labels
		labels = [subdir for _ in range(len(faces))]
		# summarize progress
		print('>loaded %d examples for class: %s' % (len(faces), subdir))
		# store
		X.extend(faces)
		y.extend(labels)
	return np.asarray(X), np.asarray(y)

# get the face embedding for one face
def get_embedding(model, face_pixels): #Runs the CNN on the images
	# scale pixel values
	face_pixels = face_pixels.astype('float32')
	# standardize pixel values across channels (global)
	mean, std = face_pixels.mean(), face_pixels.std()
	face_pixels = (face_pixels - mean) / std
	# transform face into one sample
	samples = expand_dims(face_pixels, axis=0)
	# make prediction to get embedding
	yhat = model.predict(samples)
	return yhat[0]

#testing if the person requesting access is in the database
def face_recognition(image_embedding, database):
	dist = 100 #initialize distance
	for employee in database:
		dist_candidate = np.linalg.norm(image_embedding-employee)#Calculate L2 distance between the two

		if dist_candidate < dist:
			dist = dist_candidate
	if dist > 10:
		access = "Failed Recognition"
	else:
		access = "successful minimum requirement met"#run SVM

	return access, dist

#Xtest, Ytest = load_dataset(path1+"Facenet_keras_Taniai/data/Single_test_image/")#''', number = 10))

# %% Code Execution: Loading the model
# #Matt: Parts of the code are from https://machinelearningmastery.com/
# def _main():
#     # load the model
#     pathy = "C:/Users/Matt/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/"
#     model = load_model(pathy+'/model/facenet_keras.h5')
#
# #
#
#     # %% Detecting faces
#     # load image from file
#
# #
#
#     image = PIL.Image.open(pathy + "data/images/BillGates/Bill_Gates_0000.jpg")
#     # convert to RGB, if needed
#     image = image.convert('RGB')
#     # convert to array
#     pixels = np.asarray(image)
#
# #
#
#     # create the detector, using default weights
#     detector = MTCNN()
#     # detect faces in the image
#     results = detector.detect_faces(pixels)
#     # create the detector, using default weights
#     detector = MTCNN()
#     # detect faces in the image
#     results = detector.detect_faces(pixels)
#
# #
#
#     #%%
#     # extract the bounding box from the first face
#     x1, y1, width, height = results[0]['box']
#     # bug fix
#     x1, y1 = abs(x1), abs(y1)
#     x2, y2 = x1 + width, y1 + height
#
# #
#
#     # extract the face
#     face = pixels[y1:y2, x1:x2]
#     # extract the face
#     face = pixels[y1:y2, x1:x2]
#
# #
#
#     # resize pixels to the model size
#     image = PIL.Image.fromarray(face)
#     image = image.resize((160, 160))
#     face_array = np.asarray(image)
#
# #
#
#     # %%
#     # load the photo and extract the face
#     pixels = extract_face(pathy + "data/images/BillGates/Bill_Gates_0000.jpg",required_size=(160,160))
#
# #
#
#     # %%
#     # specify folder to plot
#     folder = pathy+'data/images/5-celebrity-faces-dataset/train/ben_afflek/'
#     i = 1
#     # enumerate files
#     for filename in os.listdir(folder):
#     	# path
#     	path = folder + filename
#     	# get face
#     	face = extract_face(path)
#     	print(i, face.shape)
#     	# plot
#     	plt.subplot(2, 7, i)
#     	plt.axis('off')
#     	plt.imshow(face)
#     	i += 1
#     plt.show()
#     # %%
#     tryX, tryy = load_dataset("C:/Users/Matt/Documents/GitHub/DeepFaceRecThesis/Facenet_keras_Taniai/data/images/BillGates/")
#     # %%
#     # load train dataset
#     trainX, trainy = load_dataset(pathy+'data/images/5-celebrity-faces-dataset/train/')
#     print(trainX.shape, trainy.shape)
#     # load test dataset
#     testX, testy = load_dataset(pathy+'data/images/5-celebrity-faces-dataset/val/')
#     print(testX.shape, testy.shape)
#     # save arrays to one file in compressed format
#     np.savez_compressed(pathy+'data/images/5-celebrity-faces-dataset.npz',trainX, trainy, testX, testy)
#
# #
#
#     # %% Create face embeddings
#
# #
#
#     # load the face dataset
#     data = load(pathy+'data/images/5-celebrity-faces-dataset.npz')
#     trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']
#     print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)
#     # load the facenet model
#     model = load_model(pathy+'model/facenet_keras.h5')
#     print('Loaded Model')
#     # convert each face in the train set to an embedding
#     newTrainX = list()
#     for face_pixels in trainX:
#     	embedding = get_embedding(model, face_pixels)
#     	newTrainX.append(embedding)
#     newTrainX = asarray(newTrainX)
#     print(newTrainX.shape)
#     # convert each face in the test set to an embedding
#     newTestX = list()
#     for face_pixels in testX:
#     	embedding = get_embedding(model, face_pixels)
#     	newTestX.append(embedding)
#     newTestX = asarray(newTestX)
#     print(newTestX.shape)
#     # save arrays to one file in compressed format
#     savez_compressed(pathy+'data/images/5-celebrity-faces-embeddings.npz', newTrainX, trainy, newTestX, testy)
#
# #
#
#     # %% Recognition Execution
#     # load faces
#     data = load(pathy+'data/images/5-celebrity-faces-dataset.npz')
#     testX_faces = data['arr_2']
#     # load face embeddings
#     data = load(pathy+'data/images/5-celebrity-faces-embeddings.npz')
#     trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']
#     # normalize input vectors
#     in_encoder = Normalizer(norm='l2')
#     trainX = in_encoder.transform(trainX)
#     testX = in_encoder.transform(testX)
#     # label encode targets
#     out_encoder = LabelEncoder()
#     out_encoder.fit(trainy)
#     trainy = out_encoder.transform(trainy)
#     testy = out_encoder.transform(testy)
#     # fit model
#     model = SVC(kernel='linear', probability=True)
#     model.fit(trainX, trainy)
#     # test model on a random example from the test dataset
#     selection = choice([i for i in range(testX.shape[0])])
#     random_face_pixels = testX_faces[selection]
#     random_face_emb = testX[selection]
#     random_face_class = testy[selection]
#     random_face_name = out_encoder.inverse_transform([random_face_class])
#     # prediction for the face
#     samples = expand_dims(random_face_emb, axis=0)
#     yhat_class = model.predict(samples)
#     yhat_prob = model.predict_proba(samples)
#     # get name
#     class_index = yhat_class[0]
#     class_probability = yhat_prob[0,class_index] * 100
#     predict_names = out_encoder.inverse_transform(yhat_class)
#     print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))
#     print('Expected: %s' % random_face_name[0])
#     # plot for fun
#     plt.imshow(random_face_pixels)
#     title = '%s (%.3f)' % (predict_names[0], class_probability)
#     plt.title(title)
#     plt.show()
#
# #
#
#     return 0
